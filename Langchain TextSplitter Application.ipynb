{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a01d9d6-5eea-4cb4-ad66-0d92e4b12605",
   "metadata": {},
   "source": [
    "**LLM : Large Language Models**\n",
    "\n",
    "**Frameworks:**\n",
    "\n",
    "- ML == Scikit-Learn\n",
    "- DL == TensorFlow/PyTorch/Keras\n",
    "- NLP == NLTK\n",
    "- GenAI + LLM == LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18736a18-1fdf-4e57-9d60-bcfa0fba238d",
   "metadata": {},
   "source": [
    "- Langchain is a python coding framework (we can say high level package), it is used to integrate with LLM models.\n",
    "- LangChain is an Ocean, having 40+ companies models. Spome examples below:\n",
    "    - LLM models == ChatGPT, GeminiAI, BedRock\n",
    "\n",
    "1. ChatGPT == OpenAI + Microsoft\n",
    "2. GeminiAI == Google\n",
    "3. BedRock == Amazon\n",
    "4. Llama == Meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8a24df09-3fd7-4f1d-b240-56069ea957df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\thaku\\anaconda3\\lib\\site-packages (0.2.16)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langchain) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.38 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langchain) (0.2.40)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langchain) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langchain) (0.1.120)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langchain) (2.9.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langchain) (8.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.23.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain) (2.1)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\thaku\\anaconda3\\lib\\site-packages (0.2.17)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langchain-community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langchain-community) (2.0.34)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langchain-community) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.16 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langchain-community) (0.2.16)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.39 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langchain-community) (0.2.40)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langchain-community) (0.1.120)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langchain-community) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langchain-community) (8.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langchain<0.3.0,>=0.2.16->langchain-community) (0.2.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langchain<0.3.0,>=0.2.16->langchain-community) (2.9.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.39->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.39->langchain-community) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.39->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.112->langchain-community) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.112->langchain-community) (3.10.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain-community) (2024.6.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (1.0.5)\n",
      "Requirement already satisfied: sniffio in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.39->langchain-community) (2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.16->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.16->langchain-community) (2.23.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\thaku\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "# Step-1: Install the packages\n",
    "\n",
    "!pip install langchain\n",
    "!pip install langchain-community\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ef9aede-50c7-4f87-a745-12b73a67f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-2: Import the Packages\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "import pandas as pd\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3609acf9-a6f1-4968-bfc6-39f1122f6ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'Sample_text.txt'}, page_content='The Langchain framework is designed to help developers build applications powered by language models.\\nIt provides a variety of tools, including text splitters, to efficiently handle large documents.\\nWith TextSplitter, you can break down documents into manageable chunks, ensuring that no token limits are exceeded.\\nThis allows the language model to process large text in smaller, sequential parts, maintaining context and continuity.')]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "loader = TextLoader(\"Sample_text.txt\")\n",
    "documents = loader.load()\n",
    "print(documents)\n",
    "print(type(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eae10d-7c5a-4aba-80a4-c3d7a8458f78",
   "metadata": {},
   "source": [
    "- Document\n",
    "    - metadata\n",
    "        - page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2468d3b6-5143-47f9-a249-bf9a6b9cba60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)   # As it is a list with 1 item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9de0493-22c1-4475-93d3-e9a8b87cb9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'Sample_text.txt'}, page_content='The Langchain framework is designed to help developers build applications powered by language models.\\nIt provides a variety of tools, including text splitters, to efficiently handle large documents.\\nWith TextSplitter, you can break down documents into manageable chunks, ensuring that no token limits are exceeded.\\nThis allows the language model to process large text in smaller, sequential parts, maintaining context and continuity.')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(documents[0]))    # Here the type is langchain document\n",
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "898a83ea-7f0e-4740-9063-4a9a36bd09db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'Sample_text.txt'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata      # Source : From where this file is coming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdfe409b-9dda-431e-837f-72c846fd8e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The Langchain framework is designed to help developers build applications powered by language models.\\nIt provides a variety of tools, including text splitters, to efficiently handle large documents.\\nWith TextSplitter, you can break down documents into manageable chunks, ensuring that no token limits are exceeded.\\nThis allows the language model to process large text in smaller, sequential parts, maintaining context and continuity.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(documents[0].page_content))    # type is string\n",
    "documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bd3beea-30d8-4208-a731-311f8baf8e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "433"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c48bdf-0372-45e2-b79e-a34119dbfcae",
   "metadata": {},
   "source": [
    "- Step-3: Text Splitters also called as Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d46d8da-823d-4e80-87be-565ea862f526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa112d7e-213c-4051-8cc3-7e40e756e221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk_size     == Maximum chunk size (in characters or tokens)\n",
    "# Chunk_overlap  == Overlapping tokens between chunks to maintain continuity & to keep the semantic meaning, \n",
    "# separator      == Define how the text is split (newline, space, custom delimiter).\n",
    "\n",
    "text_splitter = CharacterTextSplitter(separator=' ',chunk_size = 120,chunk_overlap = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f38208f-ea3b-4130-9a91-69eea90c40db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Sample_text.txt'}, page_content='The Langchain framework is designed to help developers build applications powered by language models.\\nIt provides a'),\n",
       " Document(metadata={'source': 'Sample_text.txt'}, page_content='provides a variety of tools, including text splitters, to efficiently handle large documents.\\nWith TextSplitter, you can'),\n",
       " Document(metadata={'source': 'Sample_text.txt'}, page_content='you can break down documents into manageable chunks, ensuring that no token limits are exceeded.\\nThis allows the'),\n",
       " Document(metadata={'source': 'Sample_text.txt'}, page_content='allows the language model to process large text in smaller, sequential parts, maintaining context and continuity.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = text_splitter.split_documents(documents)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20f01dd-94d2-449c-ad68-4098b0aaa871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Warning while running above code, means it divided properly\n",
    "# One warning means === one chunk problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de3c8278-1035-41e7-b70e-2f7659422f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83d144ce-a649-49e3-9a9f-99162d085c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'Sample_text.txt'}, page_content='The Langchain framework is designed to help developers build applications powered by language models.\\nIt provides a')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]   # One Chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f44de0-7eb5-4c05-b8fc-0863cf1b6cc3",
   "metadata": {},
   "source": [
    "- Print all chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1169012c-f4a9-4984-be9c-ed036dae08e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk 1:\n",
      "The Langchain framework is designed to help developers build applications powered by language models.\n",
      "It provides a\n",
      "chunk 2:\n",
      "provides a variety of tools, including text splitters, to efficiently handle large documents.\n",
      "With TextSplitter, you can\n",
      "chunk 3:\n",
      "you can break down documents into manageable chunks, ensuring that no token limits are exceeded.\n",
      "This allows the\n",
      "chunk 4:\n",
      "allows the language model to process large text in smaller, sequential parts, maintaining context and continuity.\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(docs):\n",
    "    print(f\"chunk {i+1}:\\n{chunk.page_content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d793b8e-02cf-426f-8f09-c9e748c1fb64",
   "metadata": {},
   "source": [
    "- Print the lengths of all chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "237e8571-b1cb-418d-b5c4-c7d1abe5c837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[115, 120, 112, 113]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(docs[i].page_content) for i in range(len(docs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c5058387-f479-4e67-be6a-fd4b27a05eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "460"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here total length 460 > 433 because of the chunk overlap between 2 chunks\n",
    "sum([len(docs[i].page_content) for i in range(len(docs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8eba730b-2d06-4e3c-90b4-7b5dce0285bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Langchain framework is designed to help developers build applications powered by language models.\\nIt provides a',\n",
       " 'provides a variety of tools, including text splitters, to efficiently handle large documents.\\nWith TextSplitter, you can',\n",
       " 'you can break down documents into manageable chunks, ensuring that no token limits are exceeded.\\nThis allows the',\n",
       " 'allows the language model to process large text in smaller, sequential parts, maintaining context and continuity.']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = text_splitter.split_text(documents[0].page_content)      # Here we need to provide string\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d7f168e9-38f2-44f9-adb7-ffdbe435892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "\n",
    "# # Initialize OpenAI with your API key\n",
    "# llm = OpenAI(api_key=\"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\")\n",
    "\n",
    "# # Function to get a summary for a chunk of text using the ChatCompletion endpoint\n",
    "# def get_summary(chunk):\n",
    "#     response = openai.ChatCompletion.create(\n",
    "#         model=\"gpt-3.5-turbo\",  # Use a chat-based model (e.g., gpt-3.5-turbo or gpt-4)\n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#             {\"role\": \"user\", \"content\": f\"Summarize the following text:\\n\\n{chunk}\"}\n",
    "#         ],\n",
    "#         max_tokens=150,  # Limit the response length\n",
    "#         temperature=0.5  # Adjust the creativity level\n",
    "#     )\n",
    "    \n",
    "#     # Extract and return the summary text from the response\n",
    "#     return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "# # Process each chunk with the language model\n",
    "# summaries = [get_summary(chunk) for chunk in chunks]\n",
    "\n",
    "# # Print the summaries\n",
    "# for i, summary in enumerate(summaries):\n",
    "#     print(f\"Summary of Chunk {i+1}:\\n{summary}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75e69b0-421a-4fcb-b7a9-2570e373c1fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
